{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7322628e-3b78-4684-b484-69d1655015bd",
   "metadata": {},
   "source": [
    "<h1 style=\"color:green; font-family:Georgia\">\n",
    "Machine Learning for MNIST Handwritten digit recognition ::\n",
    "<hr>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a24798-70a5-4b6f-92e7-e53d9457024e",
   "metadata": {},
   "source": [
    "<span style = \"font-family:Geogia; color:orange;  font-style:italic; font-size:14px\">\n",
    "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by \"re-mixing\" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.<br>\n",
    "\n",
    "The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset. The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support-vector machine to get an error rate of 0.8%<br>\n",
    "</span>\n",
    "\n",
    "<h5 style=\"color:red; font-family:Georgia\">\n",
    "total number of images (instances): 70000 <br>\n",
    "total number of features: 784 (28$\\times$28 pixels = 784)\n",
    "</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aeb60c-93ed-44dd-a297-9a03c4062189",
   "metadata": {},
   "source": [
    "<h4 style=\"color:orange; font-family:Georgia; text-decoration: underline\">\n",
    "Importing importent modules :\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd56f7b-4275-44a1-adec-f5f210a96597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed May 22 22:18:32 2024\n",
    "@author: mrsag\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Importing modules\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import glob\n",
    "from Curve_fitting_with_scipy import Gaussianfitting as Gf\n",
    "from Curve_fitting_with_scipy import Linefitting as Lf\n",
    "from scipy.signal import fftconvolve\n",
    "from collections import defaultdict\n",
    "import PIL\n",
    "from scipy.ndimage import rotate, zoom\n",
    "from scipy import ndimage\n",
    "import math\n",
    "from skimage import io, draw\n",
    "import joblib\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder, LabelEncoder, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, ExtraTreeClassifier,ExtraTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, ExtraTreesRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, RandomForestRegressor, BaggingClassifier, BaggingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, HistGradientBoostingRegressor, StackingClassifier, StackingRegressor\n",
    "from sklearn.ensemble import VotingClassifier, VotingRegressor\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, KNeighborsTransformer, RadiusNeighborsClassifier, RadiusNeighborsRegressor\n",
    "from sklearn.metrics import root_mean_squared_error, accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LogisticRegressionCV, PassiveAggressiveClassifier, PassiveAggressiveRegressor\n",
    "from sklearn.linear_model import RidgeClassifier, RidgeClassifierCV, RidgeCV, Ridge, SGDClassifier, SGDRegressor, SGDOneClassSVM, TweedieRegressor\n",
    "from sklearn.linear_model import ARDRegression, QuantileRegressor, HuberRegressor, RANSACRegressor, TheilSenRegressor, GammaRegressor, PoissonRegressor\n",
    "from sklearn.naive_bayes import BernoulliNB, CategoricalNB, ComplementNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from xgboost import XGBRegressor, XGBClassifier, XGBRFClassifier, XGBRFRegressor\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "import openai\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "mpl.rcParams['font.serif'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 12\n",
    "mpl.rcParams['font.weight'] = 'bold'\n",
    "#mpl.rcParams['font.style'] = 'italic'  # Set this to 'italic'\n",
    "mpl.rcParams['figure.dpi'] = 120  # highres display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224d6257-7c4a-4c3d-a979-80812f21209d",
   "metadata": {},
   "source": [
    "<h4 style=\"color:orange; font-family:Georgia;text-decoration: underline\">\n",
    "Defining some required functions :\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31abe55d-fc5d-4c13-ad11-d90544894bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(array, value):\n",
    "    # Calculate the absolute differences between each element and the target value\n",
    "    absolute_diff = np.abs(array - value)\n",
    "    \n",
    "    # Find the index of the minimum absolute difference\n",
    "    index = np.argmin(absolute_diff)\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "def moving_average(signal, window_size):\n",
    "    # Define the window coefficients for the moving average\n",
    "    window = np.ones(window_size) / float(window_size)\n",
    "    \n",
    "    # Apply the moving average filter using fftconvolve\n",
    "    filtered_signal = fftconvolve(signal, window, mode='same')\n",
    "    \n",
    "    return filtered_signal\n",
    "\n",
    "\n",
    "def hist_dataframe(df, bins=10):\n",
    "    # Define a list of colors for each histogram\n",
    "    colors = ['red', 'green', 'blue', 'magenta', 'cyan', 'purple', 'orange', 'black']\n",
    "    # Create subplots with a dynamic number of rows, 3 columns per row\n",
    "    fig, axes = plt.subplots(nrows=int(np.ceil(len(df.columns) / 3)), ncols=3, figsize=(18, 4.5*int(np.ceil(len(df.columns) / 3))))\n",
    "    # Flatten the axes array for easy iteration (even if it's a 2D grid)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each histogram individually\n",
    "    for i, column in enumerate(df.columns):\n",
    "        df[column].plot(kind='hist', ax=axes[i], color=colors[i%len(colors)], title=column, bins = bins)\n",
    "        axes[i].grid(True, linewidth=0.5, color='k')  # Optional: add grid\n",
    "    \n",
    "    # Turn off any unused subplots (in case the number of columns is not a multiple of 3)\n",
    "    for j in range(i+1, len(axes)):\n",
    "        fig.delaxes(axes[j])  # Delete empty subplots\n",
    "    \n",
    "    plt.tight_layout()  # Adjust the layout\n",
    "    plt.show()\n",
    "    pass\n",
    "    \n",
    "\n",
    "def binned_mode(data, num_bins):     \n",
    "    \"\"\" use this function to replace the missing value with the most probable value in a dataset...\n",
    "        There are inbuilt functions for mean and median\"\"\"\n",
    "    # Calculate the range of the data\n",
    "    data_min, data_max = min(data), max(data)\n",
    "    \n",
    "    # Calculate the bin edges\n",
    "    bins = np.linspace(data_min, data_max, num_bins + 1)\n",
    "    \n",
    "    # Group data into bins\n",
    "    binned_data = defaultdict(list)\n",
    "    for num in data:\n",
    "        # Find the correct bin index for each number\n",
    "        bin_index = np.digitize(num, bins) - 1  # subtract 1 to get 0-based index\n",
    "        bin_index = min(bin_index, num_bins - 1)  # ensure last bin is included\n",
    "        binned_data[bin_index].append(num)\n",
    "    \n",
    "    # Find the bin with the highest frequency\n",
    "    most_frequent_bin = max(binned_data, key=lambda k: len(binned_data[k]))\n",
    "    \n",
    "    # Calculate the average of the values in the most frequent bin\n",
    "    mode_value = np.mean(binned_data[most_frequent_bin])\n",
    "    \n",
    "    return mode_value\n",
    "\n",
    "\n",
    "def plot_hollow_pillar_histogram(data, bins=30, edgecolor='black', linewidth=1.5):   #, xlabel='Value', ylabel='Frequency', title='Histogram with Hollow Pillar Bars'):\n",
    "    \"\"\"\n",
    "    Plots a histogram with hollow pillar bars.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Array of data to be plotted.\n",
    "    - bins: Number of bins or bin edges (default is 30).\n",
    "    - edgecolor: Color of the bar edges (default is 'black').\n",
    "    - linewidth: Thickness of the bar edges (default is 1.5).\n",
    "    - xlabel: Label for the x-axis (default is 'Value').\n",
    "    - ylabel: Label for the y-axis (default is 'Frequency').\n",
    "    - title: Title for the plot (default is 'Histogram with Hollow Pillar Bars').\n",
    "    \"\"\"\n",
    "    # Create the histogram without plotting it (retrieve the counts and bin edges)\n",
    "    counts, bin_edges = np.histogram(data, bins=bins)\n",
    "    # Width of each bar\n",
    "    bin_width = bin_edges[1] - bin_edges[0]\n",
    "    # Create the plot\n",
    "    for i in range(bins):\n",
    "        plt.hist(bin_edges[i]*np.ones(counts[i]),bins=1, edgecolor='black', linewidth=0.5, rwidth=(max(data)-min(data))/bins)\n",
    "        \n",
    "    # Set limits for x and y axis\n",
    "    # ax.set_xlim(bin_edges[0], bin_edges[-1])\n",
    "    # ax.set_ylim(0, max(counts) * 1.1)\n",
    "    # Add labels and title\n",
    "    # plt.xlabel(xlabel)\n",
    "    # plt.ylabel(ylabel)\n",
    "    # plt.title(title)\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def get_numerical_categorical_boolean_columns(data):\n",
    "    # Separate categorical and numerical columns\n",
    "    categorical_columns = []\n",
    "    numerical_columns = []\n",
    "    boolean_columns = []\n",
    "    \n",
    "    for name in np.array(data.columns):\n",
    "        i=0\n",
    "        while(data[name][i] is None):\n",
    "            i += 1\n",
    "\n",
    "        if(type(data[name][i]) is str):\n",
    "            categorical_columns.append(name)\n",
    "        elif((type(data[name][i]) is float) or (type(data[name][i]) is int) or (type(data[name][i]) is bin) or (type(data[name][i]) is np.int64) \n",
    "            or (type(data[name][i]) is np.int32) or (type(data[name][i]) is np.int16) or (type(data[name][i]) is np.int8) or \n",
    "            (type(data[name][i]) is np.float16) or (type(data[name][i]) is np.float32) or (type(data[name][i]) is np.float64)):\n",
    "            \n",
    "            numerical_columns.append(name)\n",
    "        elif((type(data[name][i]) is bool)):\n",
    "             boolean_columns.append(name)\n",
    "        else:\n",
    "            pass\n",
    "    return numerical_columns, categorical_columns, boolean_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e18ba-5ce6-4f4d-940d-f458c67d0153",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;font-family:Georgia\">\n",
    "Fetching the MNIST Data\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf9886d-6dfb-4813-beda-a3e7e263da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\")\n",
    "print(type(mnist))\n",
    "print(mnist.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce5ffa-7e3f-4b6c-8c61-60339c43d319",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;font-family:Georgia\">\n",
    "Splitting the data into image and target.\n",
    "</h3>\n",
    "<h4 style=\"color:orange;font-family:Georgia\">\n",
    "As the data is already shuffled, no need to shuffle and split into train and test.<br>\n",
    "But in general shuffling is needed.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb5b8e-9188-42f7-9b44-0ec8f13b8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "x , y = mnist[\"data\"], mnist['target']\n",
    "# print(mnist[\"DESCR\"])   # To Show the description\n",
    "print(f\"dataset shape: {x.shape}\")\n",
    "print(f\"target shape: {y.shape}\")\n",
    "print(f\"image shape: {np.array(x)[0].shape}\")\n",
    "\n",
    "\" Showing a random image \"\n",
    "index = np.random.randint(0, 69999)\n",
    "random_image = np.array((x.iloc[index])).reshape(28,28)\n",
    "# print(random_image)\n",
    "plt.imshow(random_image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"image no: {index};  Digit: {y[index]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b06c5-3f9a-4b49-8bd3-cb5874018fa2",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;font-family:Georgia\">\n",
    "Splitting train and test data\n",
    "</h3>\n",
    "<h4 style=\"color:orange;font-family:Georgia\">\n",
    "As the data is already shuffled, no need to shuffle and split into train and test.<br>\n",
    "But in general shuffling is needed.\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658af1b0-52ad-43cf-8096-da2ac9a944f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random permutation of indices to shuffle\n",
    "shuffle_indices = np.random.permutation(len(y))\n",
    "\n",
    "# Shuffle the features and target\n",
    "x = x.iloc[shuffle_indices]\n",
    "y = y.iloc[shuffle_indices]\n",
    "\n",
    "train_percent = int(len(y)*0.8)\n",
    "\n",
    "x_train, y_train = np.array(x[:train_percent]), np.array(y[:train_percent])\n",
    "x_test, y_test = np.array(x[train_percent:]), np.array(y[train_percent:])\n",
    "\n",
    "y_train, y_test = y_train.astype(np.int8), y_test.astype(np.int8)\n",
    "\n",
    "print(f\"Train data size: {x_train.shape}\\nTest data size: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11302201-30b3-4b73-865f-e7d9dd1394ba",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;font-family:Georgia\">\n",
    "Fitting the model: \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ae1aed-e1be-4df8-bbc7-ff0d1f7e04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression()\n",
    "# model = RandomForestClassifier()\n",
    "# model = DecisionTreeClassifier()\n",
    "model = KNeighborsClassifier()\n",
    "# model = ExtraTreesClassifier()\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "# cross_val_scores = cross_val_score(model, x_train, y_train, scoring=\"accuracy\", cv=10)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "print(f\"accuracy_score: {accuracy_score(y_test, y_predict)*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431e6ba9-edc4-4ce6-8d9e-07952c6f518f",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;font-family:Georgia\">\n",
    "plotting one random test data\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0280d5f7-3059-4ac4-b2e0-e3fcb59076f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.randint(len(y_test)-1)\n",
    "\n",
    "random_image = np.array((x_test[index])).reshape(28,28)\n",
    "# print(random_image)\n",
    "plt.imshow(random_image, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"index: {index};  Digit: {y_test[index]};  predict: {y_predict[index]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef9f1d9-43a6-4304-9434-0ae8d223b402",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;font-family:Georgia\">\n",
    "Checking the prcentage of the incorrect predictions for different digits...\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c100e17-a883-406b-96ad-a8f0e3f6d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify incorrect predictions\n",
    "incorrect_indices = np.where(y_test != y_predict)[0]\n",
    "incorrect_labels = y_test[incorrect_indices]# Extract the corresponding true labels (y_test) for incorrect predictions\n",
    "total_per_digit = np.bincount(y_test, minlength=10) # Calculate the total occurrences of each digit in the test set\n",
    "incorrect_per_digit = np.bincount(incorrect_labels, minlength=10)  # Calculate the incorrect occurrences for each digit\n",
    "percentage_incorrect_per_digit = (incorrect_per_digit / total_per_digit) * 100  # Calculate the percentage of incorrect predictions for each digit\n",
    "\n",
    "# Plot the percentage of incorrect predictions for each digit\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.bar(np.arange(len(set(y_test))), percentage_incorrect_per_digit, color='red', edgecolor='black', linewidth = 2, width=0.5)\n",
    "plt.xticks(np.arange(10))  # Digits 0 to 9\n",
    "plt.xlabel('True Digit Label')\n",
    "plt.ylabel('Incorrect Predictions (%)')\n",
    "plt.title('Incorrect Predictions (in %) for Each Digit\\n'+f\"average: {np.mean(percentage_incorrect_per_digit):.2f} %\", fontsize=10, fontweight='bold')\n",
    "plt.grid(True, lw=1, color = 'k')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba14fa0-0689-447d-bef3-bd123d22e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_digit(image_path):\n",
    "  \"\"\"\n",
    "  Loads an image, resizes it to 28x28, flattens it, and predicts the digit using the trained model.\n",
    "\n",
    "  Args:\n",
    "    image_path: The path to the image file.\n",
    "\n",
    "  Returns:\n",
    "    The predicted digit.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    img = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    img = img.resize((28, 28))  # Resize to 28x28\n",
    "    img_array = np.array(img).flatten()  # Flatten the image array\n",
    "    prediction = model.predict([img_array])[0]  # Predict using the loaded model\n",
    "    return prediction\n",
    "  except Exception as e:\n",
    "    print(f\"Error processing image: {e}\")\n",
    "    return None\n",
    "\n",
    "# Save the function and the model to a joblib file\n",
    "# joblib.dump((predict_digit, model), 'digit_predictor.joblib') \n",
    "# Save the model to a file using joblib\n",
    "\n",
    "# joblib.dump(model, 'digit_predictor.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
